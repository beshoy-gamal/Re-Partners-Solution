
# High-Level Solution Design

Here I designed the solution End to End with Technical Components Including GCP, Source Systems, and Automation.

<img width="700" height="500" alt="image" src="https://github.com/user-attachments/assets/28fb727c-dbb4-4eb5-ab94-ad277c3405f9" />

# 1\. Solution Components 
This high-level solution design outlines the major technical components involved in the system architecture. The design leverages Google Cloud Platform (GCP) services, integrates with source systems, and employs automation using GitHub Actions and Terraform for streamlined deployment and management.
# 2\. The Solution Workflow
1. Data or events are generated by the source systems.
1. Source data is securely ingested into GCP services (Through Pub/Sub).
1. Processing occurs using GCP DataFlow
1. Processed data is stored in BigQuery and GCS
1. Data Vizualized for the users by Dashboard created on Looker 
1. Automation workflows (triggered via GitHub Actions) manage code deployments and infrastructure updates using Terraform, ensuring all changes are tracked and auditable.
1. Monitoring and logging are handled through GCP’s operations suite for observability and incident response.
# 3\. My assumption for Security and Best Practices
- All credentials and secrets are managed securely using GCP Secret Manager and GitHub repository secrets.
- IAM roles are defined with the principle of least privilege to restrict resource access.
- Audit logs and monitoring are enabled for all critical components.
- Code reviews and automated tests are enforced within the GitHub Actions pipeline.



# Steps of the Implementation
  1. Create the PubSub Topic
  
  <img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/0d7272ef-b7c1-45d4-a98f-e95db530d34f" />

  
  
  2. Create the PubSub Subscription
  
  <img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/c99212b9-69c1-4eaf-ae0c-e9604aa544d4" />

  
  3. Using the DDL SQL files in the BigQuery-DDL Folder, created here 3 tables for different event Types 
     - Here the Schema of each table

        <img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/7ec16793-a418-4fab-a48f-c39152b9d874" />
        

        <img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/a3c214ba-bdfe-400c-9e96-887462f84b46" />
        

        <img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/fa0aa7ee-364e-4a9b-9f7a-a78b270fc8b6" />

.
  	- Here the table under the BigQuery DataSet

  <img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/d0778394-35d9-44df-b689-39ec68f9497a" />
 

  4. Using the Python file dataflow_pipline.py, there are Apache Beam Data Pipline, here screen from runnung the pipline
  
     <img width="1200" height="500" alt="image" src="https://github.com/user-attachments/assets/814b07a6-fdf5-4fc9-a86d-0ce02fb2d675" />

  
  
  
  5. Sending samples of meassages with the same schema for the created PubSub topic
  
  <img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/cd3c36f7-e24d-485d-b89e-29fe9ace7cef" />

  
  6. Here the data after procced by DataFlow and stored in Bigquery
  
  <img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/ea4128c1-c85d-4c67-a3d0-d36662e60fae" />

  
  7. Here the data after procced by DataFlow and stored in GCS 
  
  <img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/a8700de4-efe7-408a-8f50-c81a088a88d3" />




## Important Consideration and design decisions:
- For Value enforcing like *"status": "enum(pending, processing, shipped, delivered)",*

  As BigQuery not have Strict functionality, I have applied that in the dataprocessing pipline on DataFlow, so we will have make sure that the data ingested in BigQuery table have the required validations

	<img width="468" height="139" alt="image" src="https://github.com/user-attachments/assets/8626f1ad-037f-4e03-aa1b-ea545cfcc100" />


- For the requirment of sending all messages in one PubSub topic, here the Pipline need to identify every message type to store the results in the targeted BigQuery Table

  So I have create a function to filter depending on the “event\_type” value, and next applyied Beam.Partition to split the messages in differt streams\
  
  <img width="468" height="236" alt="image" src="https://github.com/user-attachments/assets/167952c8-0005-4975-8ffa-b80e8a2e01d5" />
	</br>

  <img width="468" height="125" alt="image" src="https://github.com/user-attachments/assets/456fc1c8-ef8e-48e4-9d0f-46b0dfa9bf1e" />


- For the requirment  **“*Tracking historical data and time travel*”**
	- There are two solution depending on time retention required to trace\
	  	- Native BigQuery Time Travel 
	
	  	** BigQuery automatically stores the history of your table changes for 7 days This allows to query the table exactly as it looked in the past.
	  	** Example:
			-- Query the table as it existed 1 hour ago
			SELECT * FROM `streaming_pubsub_dataflow.orders`
			FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR);
		
		- Permanent History Tracking
		
		 ** Here need to add two columns to every table: valid\_from and valid\_to.
		
		 ** But that need to be consider as it will change the logic of ingestion and data quering
	
	     ** Example:
				SELECT *
				FROM `streaming_pubsub_dataflow.orders`
				WHERE valid_from <= '2025-10-25 10:00:00'
				AND (valid_to > '2023-11-20 10:00:00' OR valid_to IS NULL);
   
-  For the requirment of storing the data in GCS Bucket in Hierarchical structure path:
  	* I have create the below function to idenitify the path Hierarchical, and use Apache Beam GCS WRITEFILE functionality
     	<img width="913" height="439" alt="image" src="https://github.com/user-attachments/assets/57439cfd-839c-48a4-9f69-c715ad4b5e0a" />

		<img width="936" height="337" alt="image" src="https://github.com/user-attachments/assets/ed4a0094-b084-4b83-a97c-71b54519d8d5" />

- for the requirment "Partitioning and clustering strategies"
	** It is considered in BigQuery DDL files

	** My logic is to use the day (of the message time staom as a partion to optmize future queries),
	And use business identifier to the clusering so in the future queries for adding WHERE in SQL will reduce the data amount processing that will achieve more optmization
		



**\


