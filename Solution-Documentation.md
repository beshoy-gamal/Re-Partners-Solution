
# High-Level Solution Design

Here I designed the solution End to End with Technical Components Including GCP, Source Systems, and Automation.

<img width="700" height="500" alt="image" src="https://github.com/user-attachments/assets/28fb727c-dbb4-4eb5-ab94-ad277c3405f9" />

# 1\. Solution Components 
This high-level solution design outlines the major technical components involved in the system architecture. The design leverages Google Cloud Platform (GCP) services, integrates with source systems, and employs automation using GitHub Actions and Terraform for streamlined deployment and management.
# 2\. The Solution Workflow
1. Data or events are generated by the source systems.
1. Source data is securely ingested into GCP services (Through Pub/Sub).
1. Processing occurs using GCP DataFlow
1. Processed data is stored in BigQuery and GCS
1. Data Vizualized for the users by Dashboard created on Looker 
1. Automation workflows (triggered via GitHub Actions) manage code deployments and infrastructure updates using Terraform, ensuring all changes are tracked and auditable.
1. Monitoring and logging are handled through GCP’s operations suite for observability and incident response.
# 3\. My assumption for Security and Best Practices
- All credentials and secrets are managed securely using GCP Secret Manager and GitHub repository secrets.
- IAM roles are defined with the principle of least privilege to restrict resource access.
- Audit logs and monitoring are enabled for all critical components.
- Code reviews and automated tests are enforced within the GitHub Actions pipeline.



# Steps of the Implementation
  1. Create the PubSub Topic
  
  ![A screenshot of a computer&#x0A;&#x0A;AI-generated content may be incorrect.](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.002.png)
  
  
  2. Create the PubSub Subscription
  
  
  **![](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.003.png)**
  
  3. Using the DDL SQL files in the attached ZIP file, created here 3 tables for different event Types 
     - Here the Schema of each table 
  
  
  
  
  ![](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.004.png)
  
  ![](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.005.png)
  
  ![A screenshot of a computer&#x0A;&#x0A;AI-generated content may be incorrect.](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.006.png)
  	- Here the table under the BigQuery DataSet
  
  

  4. Using the Python file in the attached ZIP folder, there are Apache Beam Data Pipline, here screen from runnung the pipline
  
     ![A screenshot of a computer&#x0A;&#x0A;AI-generated content may be incorrect.](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.008.png)
  
  
  
  5. Sending samples of meaasages with the same schema for the created PubSub topic
  
  ![A screenshot of a computer&#x0A;&#x0A;AI-generated content may be incorrect.](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.009.png)
  
  6. Here the data after procced by DataFlow and stored in Bigquery
  
  ![](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.010.png)
  
  7. Here the data after procced by DataFlow and stored in GCS 
  
  ![](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.011.png)



## Important Consideration and design decisions:
- For Value enforcing like *"status": "enum(pending, processing, shipped, delivered)",*

  As BigQuery not have Strict functionality, I have applied that in the dataprocessing pipline on DataFlow, so we will have make sure that the data ingested in BigQuery table have the required validations

  ![A computer code with text&#x0A;&#x0A;AI-generated content may be incorrect.](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.012.png)

- For the requirment of sending all messages in one PubSub topic, here the Pipline need to identify every message type to store the results in the targeted BigQuery Table

  So I have create a function to filter depending on the “event\_type” value, and next applyied Beam.Partition to split the messages in differt streams\
  ![A computer screen shot of a program&#x0A;&#x0A;AI-generated content may be incorrect.](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.013.png)

  ![A computer code with colorful text&#x0A;&#x0A;AI-generated content may be incorrect.](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.014.png)

- For the requirment  **“*Tracking historical data and time travel*”**
- There are two solution depending on time retention required to trace\
      - Native BigQuery Time Travel 

BigQuery automatically stores the history of your table changes for 7 days This allows to query the table exactly as it looked in the past.

\- Permanent History Tracking

`	`Here need to add two columns to every table: valid\_from and valid\_to.

`	`But that need to be consider as it will change the logic of ingestion and data quering 	

**\


![C2 General](Aspose.Words.83ea0252-a684-433f-9a34-ecb7d13eb127.015.png)
