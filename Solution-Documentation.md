
High-Level Solution Design
Here I designed the solution End to End with Technical Components Including GCP, Source Systems, and Automation.
 <img width="512" height="339" alt="image" src="https://github.com/user-attachments/assets/26219b13-bace-4244-9d31-d6b0f389be29" />


1. HLD Solution
    This high-level solution design outlines the major technical components involved in the system architecture. The design leverages Google Cloud Platform (GCP) services, integrates with source systems, and employs automation using GitHub Actions and Terraform for streamlined deployment and management.
2. The Solution Workflow
    1.	Data or events are generated by the source systems.
    2.	Source data is securely ingested into GCP services (Through Pub/Sub).
    3.	Processing occurs using GCP DataFlow
    4.	Processed data is stored in BigQuery and GCS
    5.	Data Vizualized for the users by Dashboard created on Looker 
    6.	Automation workflows (triggered via GitHub Actions) manage code deployments and infrastructure updates using Terraform, ensuring all changes are tracked and auditable.
    7.	Monitoring and logging are handled through GCP’s operations suite for observability and incident response.
3. My assumption for Security and Best Practices
    •	All credentials and secrets are managed securely using GCP Secret Manager and GitHub repository secrets.
    •	IAM roles are defined with the principle of least privilege to restrict resource access.
    •	Audit logs and monitoring are enabled for all critical components.
    •	Code reviews and automated tests are enforced within the GitHub Actions pipeline.













	



Steps of the Implementation
  1-	Create the PubSub Topic
   
  
  
  2-	Create the PubSub Subscription
  
  
   
  
  3-	Using the DDL SQL files in the attached ZIP file, created here 3 tables for different event Types 
  	Here the Schema of each table 
  
  
  
  
   
  
   
  
   
  
  
  4-	Here the table under the BigQuery DataSet
  
  5-	Using the Python file in the attached ZIP folder, there are Apache Beam Data Pipline, here screen from runnung the pipline
  
  
  
  
  6-	Sending samples of meaasages with the same schema for the created PubSub topic
  
   
  7-	Here the data after procced by DataFlow and stored in Bigquery
  
  8-	Here the data after procced by DataFlow and stored in GCS 

 



Important Consideration and design decisions:
•	For Value enforcing like "status": "enum(pending, processing, shipped, delivered)",
As BigQuery not have Strict functionality, I have applied that in the dataprocessing pipline on DataFlow, so we will have make sure that the data ingested in BigQuery table have the required validations
 
•	For the requirment of sending all messages in one PubSub topic, here the Pipline need to identify every message type to store the results in the targeted BigQuery Table
So I have create a function to filter depending on the “event_type” value, and next applyied Beam.Partition to split the messages in differt streams
 
 

•	For the requirment  “Tracking historical data and time travel”
•	There are two solution depending on time retention required to trace
  - Native BigQuery Time Travel 
      BigQuery automatically stores the history of your table changes for 7 days This allows to query the table exactly as it looked in the past.
  - Permanent History Tracking
    	Here need to add two columns to every table: valid_from and valid_to.
    	But that need to be consider as it will change the logic of ingestion and data quering 	



